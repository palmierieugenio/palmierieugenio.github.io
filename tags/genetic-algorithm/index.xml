<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Genetic Algorithm on Computation Debate</title>
    <link>/./tags/genetic-algorithm/</link>
    <description>Recent content in Genetic Algorithm on Computation Debate</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2017 Eugenio Palmieri</copyright>
    <lastBuildDate>Sat, 14 Sep 2019 00:00:00 +0000</lastBuildDate>
    <atom:link href="/tags/genetic-algorithm/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Regression using Genetic Algorithm</title>
      <link>/./post/regression-using-genetic-algorithm/</link>
      <pubDate>Sat, 14 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>/./post/regression-using-genetic-algorithm/</guid>
      <description>&lt;p&gt;The idea of this post is to solve (or at least approximate) the solution to Ordinary Least Squares (OLS), Least Absolute Deviation (LAD), Quantile Regression, Lasso and &lt;span class=&#34;math inline&#34;&gt;\(L0\)&lt;/span&gt; regularization using Genetic Algorithm and the &lt;a href=&#34;https://cran.r-project.org/web/packages/GA/vignettes/GA.html&#34;&gt;&lt;code&gt;GA&lt;/code&gt; package&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;creating-the-data&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Creating the data&lt;/h1&gt;
&lt;p&gt;I create data with some X non correlated with the response Y.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Packages
library(GA)
library(L1pack)
library(L0Learn)
library(quantreg)
library(glmnet)

# Problem data
set.seed(1)
p &amp;lt;- 8
n &amp;lt;- 5000
DENSITY &amp;lt;- 0.5   # Fraction of non-zero beta
beta.mean&amp;lt;-10; intercept&amp;lt;-13
beta_true &amp;lt;- matrix(rnorm(p, beta.mean), ncol = 1)
idxs &amp;lt;- sample.int(p, size = floor((1 - DENSITY) * p), replace = FALSE)
beta_true[idxs] &amp;lt;- 0
X &amp;lt;- matrix(rnorm(n * p, sd = 5), nrow = n, ncol = p)
sigma &amp;lt;- 20
eps &amp;lt;- matrix(rnorm(n, sd = sigma), ncol = 1)
beta_true&amp;lt;-c(intercept, beta_true)
X.intercept&amp;lt;-cbind(1, X)
Y &amp;lt;- X.intercept %*% beta_true + eps
K &amp;lt;- as.numeric(ncol(X))
m &amp;lt;- length(Y)

X&amp;lt;-scale(X) # standardize for regularization techniques&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;ordinary-least-squares-ols&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Ordinary Least Squares (OLS)&lt;/h1&gt;
&lt;p&gt;The solution could be easily calculated in closed form using &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}_{OLS}= (X^T X)^{-1} X^T y\)&lt;/span&gt;, which gives:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;beta.ols&amp;lt;-function(xx=X, yy=Y){
        xx&amp;lt;-cbind(1, xx)
        beta&amp;lt;-solve(t(xx)%*%xx)%*%t(xx)%*%yy
        beta&amp;lt;-as.numeric(beta)
}

ols.beta&amp;lt;-beta.ols()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this case using Genetic Algorithm is defenitely inefficient and less precise. The same is true for the other models in this post: they don’t have generally a solution in closed form, but they have more robust numerical methods that take advantage of the structure of the problems.&lt;/p&gt;
&lt;p&gt;To solve the problems with the package &lt;code&gt;GA&lt;/code&gt; we need to specify some parameters:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;max.iter&amp;lt;-2000 # max number of iteration
max.run&amp;lt;-300 # number of iteration without any improvement before stopping
theta&amp;lt;-rep(0,p) # empty vector of suggested parameters without intercept
iper=0.01 # iperparameter for regularization methods
up&amp;lt;-rep(100, p+1) # maximum values of parameters
low&amp;lt;- -up # minimum values of parameters&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can solve OLS, minimizing directly the function:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\min_{\beta} \sum_{i=1}^n (y_i - x_i^T \beta)^2 \]&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# negative sign, since ga actually maximize the function
ols.func&amp;lt;-function(beta, xx=X, yy=Y){  
        xx&amp;lt;-cbind(1, xx)
        J &amp;lt;- -sum((xx%*%beta- yy)^2)
        return(J) 
}
GA.ols &amp;lt;- ga(type = &amp;quot;real-valued&amp;quot;, fitness = ols.func,
         lower = low, upper = up, suggestions = c(0,theta),
        popSize = 300, maxiter = max.iter, run = max.run, monitor = NULL)

GA.ols&amp;lt;-as.numeric(summary(GA.ols)$solution)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;least-absolute-deviation-lad&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Least Absolute Deviation (LAD)&lt;/h1&gt;
&lt;p&gt;The Least Absolute Deviation (LAD) doesn’t have a solution in closed form, but can be solved with numerical methods, like Barrodale and Roberts algorithm (the default of the &lt;code&gt;L1pack&lt;/code&gt; package used in this post) or EM:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# pacchetto L1pack
lad.package&amp;lt;-lad(Y~X, data=list(X,Y), method = &amp;quot;BR&amp;quot;)$coefficients&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can solve LAD, minimizing directly the function:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\min_{\beta} \sum_{i=1}^n |y_i - x_i^T \beta| \]&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# negative sign, since ga actually maximize the function
lad.func&amp;lt;-function(beta, xx=X, yy=Y){  
  xx&amp;lt;-cbind(1, xx)
  lad&amp;lt;-sum(abs(yy-xx%*%beta))
  J&amp;lt;- - lad
  return(J) 
}

GA.lad &amp;lt;- ga(type = &amp;quot;real-valued&amp;quot;, fitness = lad.func,
               lower = low, upper =up,
              suggestions=c(0,theta),
               popSize = 300, maxiter = max.iter,
             run = max.run, monitor = NULL)

GA.lad&amp;lt;-as.numeric(summary(GA.lad)$solution)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;quantile-regression&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Quantile Regression&lt;/h1&gt;
&lt;p&gt;Also quantile regression can be solved with several numerical methods: the default one used by &lt;code&gt;quantreg&lt;/code&gt; is a modified version of the Barrodale and Roberts algorithm for l1-regression described in detail in Koenker and d’Orey (1987, 1994). For larger problems it is advantageous to use the Frisch-Newton interior point method or other variants described in detail in Portnoy and Koenker(1997).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;quantile.fit&amp;lt;-rq(Y~X, data=list(X,Y), tau = c(0.05, 0.5, 0.95), method = &amp;quot;br&amp;quot;)
quantile.fit&amp;lt;-t(quantile.fit$coefficients)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The loss of quantile regression can be calculated using the &lt;em&gt;check function&lt;/em&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\rho(x, \ \tau)=
\begin{cases}
\tau x  &amp;amp; \quad \text{if } x \geq 0\\
(1-\tau) x  &amp;amp; \quad \text{if } x &amp;lt; 0\\
\end{cases}\]&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;check.function&amp;lt;-function(z, t=0.5){
  risultato&amp;lt;-ifelse(z&amp;gt;=0, t*z, (t-1)*z)
  risultato[z==0]&amp;lt;-0
  risultato
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then we can define the loss function of the quantile regression as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\min_{\beta} \frac{1}{n} \sum_{i=1}^n \rho(y_i - x_i^T \beta, \ \tau)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Where the constant &lt;span class=&#34;math inline&#34;&gt;\(\frac{1}{n}\)&lt;/span&gt; can be omitted from the optimization procedure, since it doesn’t depend on &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;quant_loss &amp;lt;- function(beta, xx=X, yy=Y, tau=0.5) { 
  xx&amp;lt;-cbind(1, xx)
  error&amp;lt;-yy-xx%*%beta
  loss&amp;lt;-check.function(error, t=tau)
  -sum(loss)
}

GA.quant &amp;lt;- ga(type = &amp;quot;real-valued&amp;quot;, fitness = quant_loss, tau=0.5,
             lower = low, upper = up,
             suggestions = quantile.fit[1,],
             popSize = 300, maxiter = max.iter, run = max.run, monitor = NULL)

GA.quant.50&amp;lt;-as.numeric(summary(GA.quant)$solution)

GA.quant.95 &amp;lt;- ga(type = &amp;quot;real-valued&amp;quot;, fitness = quant_loss, tau=0.95,
               lower = low, upper =up,
               suggestions = quantile.fit[2,],
               popSize = 300, maxiter = max.iter, run = max.run, monitor = NULL)

GA.quant.95&amp;lt;-as.numeric(summary(GA.quant.95)$solution)

GA.quant.05 &amp;lt;- ga(type = &amp;quot;real-valued&amp;quot;, fitness = quant_loss, tau=0.05,
                  lower = low, upper = up,
                  suggestions = quantile.fit[3,],
                  popSize = 300, maxiter = max.iter, run = max.run, monitor = NULL)

GA.quant.05&amp;lt;-as.numeric(summary(GA.quant.05)$solution)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;lasso---l1-penalization&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Lasso - &lt;span class=&#34;math inline&#34;&gt;\(L1\)&lt;/span&gt; Penalization&lt;/h1&gt;
&lt;p&gt;The Least Absoulute Shrinkage and Selection Operator (Lasso) problem can be solved with the &lt;code&gt;glmnet&lt;/code&gt; package that uses cyclical coordinate descent algorithms, which successively optimizes the objective function over each parameter with others fixed, and cycles repeatedly until convergence:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit.glmnet.lasso&amp;lt;-glmnet(X, Y, family =&amp;quot;gaussian&amp;quot;, lambda = iper)
lasso.glmnet&amp;lt;-as.numeric(coef(fit.glmnet.lasso))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The loss can be minimized directly:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\min_\beta \frac{1}{2n} \sum_{i=1}^n (y_i - \beta_0 - \sum_{j=1}^p x_{ij} \beta_j )^2 + \lambda \sum_{j=1}^p |\beta_j|\]&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lasso.loss&amp;lt;-function(beta, xx=X, yy=Y, lambda=iper){  
        const&amp;lt;-1/(length(yy)*2)
        # xx&amp;lt;-scale(xx) # already normalized previously!
        ols&amp;lt;-sum((yy-xx%*%beta)^2)
        J &amp;lt;- -((const*ols)+(lambda*sum(abs(beta)) ))
        return(J) 
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The X must standardized before optimizing the function. The intercept is not used in the optimization procedure and it’s estimated using:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\hat{\beta}_0= \bar{y} -  \sum_{j=1}^p \bar{x}_j \hat{\beta}_j\]&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;intercept.mean&amp;lt;-function(beta, xx=X, yy=Y){
  intercept&amp;lt;-mean(yy)-apply(xx,2,mean)%*%beta
  intercept
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So the full solution is give by:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;GA.lasso &amp;lt;- ga(type = &amp;quot;real-valued&amp;quot;, fitness = lasso.loss,
               lower = low[-1], upper = up[-1],
               suggestions=rbind(theta, lasso.glmnet[-1]),
               popSize = 300, maxiter = max.iter,
               run = max.run, monitor = NULL)


GA.lasso&amp;lt;-as.numeric(summary(GA.lasso)$solution)
GA.lasso&amp;lt;-c(intercept.mean(GA.lasso), GA.lasso)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;zero-norm-l0-regularization&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Zero Norm L0 Regularization&lt;/h1&gt;
&lt;p&gt;The L0 regularization is very similar, but it penalizes the &lt;span class=&#34;math inline&#34;&gt;\(\beta \neq 0\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\min_\beta \frac{1}{2n} \sum_{i=1}^n (y_i - \beta_0 - \sum_{j=1}^p x_{ij} \beta_j )^2 + \lambda \sum_{j=1}^p I_{\beta_j \neq 0}(\beta_j)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Where &lt;span class=&#34;math inline&#34;&gt;\(I_{beta_j \neq 0}(x)\)&lt;/span&gt; returns 1 if &lt;span class=&#34;math inline&#34;&gt;\(\beta_j \neq 0\)&lt;/span&gt; and 0 otherwise.&lt;/p&gt;
&lt;p&gt;The problem can be solved with the package &lt;code&gt;L0Learn&lt;/code&gt; which uses a variant of cyclic coordinate descent as default or a local combinatorial search on top of coordinate descent (typically achieves higher quality solutions at the expense of increased running time).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# L0 regularization L0Learn package
fit.L0&amp;lt;-L0Learn.fit(X, Y, penalty=&amp;quot;L0&amp;quot;, autoLambda=FALSE, lambdaGrid=list(iper),
                    loss=&amp;quot;SquaredError&amp;quot;, algorithm = &amp;quot;CD&amp;quot;)
L0.package&amp;lt;-as.numeric(coef(fit.L0))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The loss can be minimized directly using &lt;code&gt;GA&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;L0.loss&amp;lt;-function(beta, xx=X, yy=Y, lambda=iper){  
  const&amp;lt;-1/length(yy)
  ols&amp;lt;-sum((yy-xx%*%beta)^2)
  J &amp;lt;- -((const*ols)+(lambda*sum(beta!=0) ))
  return(J) 
}

GA.zero.norm &amp;lt;- ga(type = &amp;quot;real-valued&amp;quot;, fitness = L0.loss,
             lower = low[-1], upper = up[-1],
             suggestions=rbind(theta, lasso.glmnet[-1], L0.package[-1]),
             popSize = 300, maxiter = max.iter, run = max.run, monitor = NULL)

GA.zero.norm&amp;lt;-as.numeric(summary(GA.zero.norm)$solution)
GA.zero.norm&amp;lt;-c(intercept.mean(GA.zero.norm), GA.zero.norm)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;results&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Results&lt;/h1&gt;
&lt;p&gt;We can compare the results of &lt;code&gt;GA&lt;/code&gt; with the true beta (since we know the data generation process) and with theier packages counterpart:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;beta.problema&amp;lt;-rbind(beta_true=beta_true,
                     ols.beta=ols.beta,
                     GA.ols=GA.ols,
                     lad.package=lad.package,
                     GA.lad=GA.lad,
                     quantile.fit,
                     GA.quant.05=GA.quant.05,
                     GA.quant.50=GA.quant.50,
                     GA.quant.95=GA.quant.95,
                     lasso.glmnet=lasso.glmnet,
                     GA.lasso=GA.lasso,
                     L0.package=L0.package,
                     GA.zero.norm=GA.zero.norm)

round(beta.problema,2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##              (Intercept)    X1    X2    X3    X4    X5    X6    X7    X8
## beta_true          13.00  0.00  0.00  9.16 11.60 10.33  0.00 10.49  0.00
## ols.beta           11.30  0.43 -0.10 46.21 57.34 53.04 -0.24 52.34  0.09
## GA.ols             11.33  0.37 -0.10 46.14 57.31 52.85 -0.28 52.25  0.13
## lad.package        10.99  0.41 -0.48 46.42 57.20 52.83 -0.37 52.65  0.12
## GA.lad             10.94  0.41 -0.45 46.44 57.16 52.74 -0.36 52.64  0.15
## tau= 0.05         -22.26  1.40  0.59 45.40 57.32 53.27 -0.53 52.32 -0.65
## tau= 0.50          10.99  0.41 -0.48 46.42 57.20 52.83 -0.37 52.65  0.12
## tau= 0.95          44.48 -0.10  0.15 45.98 56.80 52.67  0.04 51.87  0.93
## GA.quant.05       -22.28  1.37  0.58 45.23 57.26 53.34 -0.46 52.23 -0.71
## GA.quant.50        11.00  0.41 -0.47 46.42 57.18 52.81 -0.35 52.63  0.13
## GA.quant.95        44.47 -0.06  0.16 45.87 56.77 52.62  0.03 51.86  0.96
## lasso.glmnet       11.30  0.42 -0.09 46.20 57.33 53.03 -0.23 52.33  0.08
## GA.lasso           11.30  0.42 -0.09 46.20 57.33 53.03 -0.23 52.33  0.08
## L0.package         11.30  0.00  0.00 46.20 57.34 53.04  0.00 52.34  0.00
## GA.zero.norm       11.30  0.42 -0.10 46.19 57.33 53.03 -0.23 52.33  0.08&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;All the results of &lt;code&gt;GA&lt;/code&gt; are close to their package counterpart, exluded the zero norm and the package &lt;code&gt;L0Learn&lt;/code&gt;, that tends to have a more sparse solution. This could be due to a numerical approximation or just a different parametrization, since the package authors didn’t write the full mathematical formula used in the package. If that is the case, we are optimizing different functions, where the iperparamer &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; may have a different meaning.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;Genetic Algorithms can be very useful when we don’t have a solution in closed form or when we can’t employ other more consistent numerical methods (expecially for non-convex problems). This article is mostly an exercise to the use of Genetic Algorithm in regression, but the real advantage of &lt;code&gt;GA&lt;/code&gt; can appear with less trivial losses to minimize.&lt;/p&gt;
&lt;p&gt;At the same time, we have shown that &lt;code&gt;GA&lt;/code&gt; can give a good approximation of other numerical methods, but it is very slow and it is generally better to employ more &lt;em&gt;smart&lt;/em&gt; solutions.&lt;/p&gt;
&lt;p&gt;Genetic Algorithms can be a useful first step to try to solve problems we don’t fully understand, a smart use of brute force.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
